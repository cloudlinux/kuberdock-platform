#!/bin/bash
# install kubernetes components on Node host

echo "Set locale to en_US.UTF-8"
export LANG=en_US.UTF-8

# TODO change rules, not disable
echo "Setting up firewall rules..."
systemctl stop firewalld
systemctl disable firewalld

echo "Using MASTER_IP={{ master_ip }}"



# 1. create yum repo file

cat > /etc/yum.repos.d/kube-cloudlinux.repo << EOF
[kube]
name=kube
baseurl=http://repo.cloudlinux.com/kubernetes/x86_64/
enabled=1
gpgcheck=1
gpgkey=http://repo.cloudlinux.com/cloudlinux/security/RPM-GPG-KEY-CloudLinux
EOF



# 1.1 import CloudLinux key
rpm --import http://repo.cloudlinux.com/cloudlinux/security/RPM-GPG-KEY-CloudLinux



# 2. install components
echo "Installing kubernetes..."
#yum -y install kubernetes
yum -y install http://repo.cloudlinux.com/kubernetes/x86_64/kubernetes-0.12.0-0.1.gitecca426.el7.centos.x86_64.rpm
yum -y install flannel



# 3. configure Node config
echo "Configuring services..."
cat > /etc/kubernetes/config << EOF
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kubernetes-apiserver.service
#   kubernetes-controller-manager.service
#   kubernetes-scheduler.service
#   kubelet.service
#   kubernetes-proxy.service

# Comma seperated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS="--etcd_servers=http://{{ master_ip }}:4001"

# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privleged docker containers
KUBE_ALLOW_PRIV="--allow_privileged=false"
EOF



# 4. configure Node kubelet

cat > /etc/kubernetes/kubelet << EOF
###
# kubernetes kubelet (Node) config

# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=0.0.0.0"

# The port for the info server to serve on
KUBELET_PORT="--port=10250"

# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME=""

# location of the api-server
KUBELET_API_SERVER="--api_servers={{ master_ip }}:8080"

# Additional configuration for using manifests on local node
KUBELET_ARGS="--config=/etc/kubernetes/manifests"
EOF



# 5. configure Flannel

cat > /etc/sysconfig/flanneld << EOF
# Flanneld configuration options

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD="http://{{ master_ip }}:4001"

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
FLANNEL_ETCD_KEY="/kuberdock/network/"

# Any additional options that you want to pass
FLANNEL_OPTIONS="--iface={{ inet_iface }}"
EOF



# 6. Setting kernel parameters
sysctl -w net.ipv4.ip_nonlocal_bind=1
cat > /etc/sysctl.d/75-kuberdock.conf << EOF
net.ipv4.ip_nonlocal_bind = 1
EOF



# 7. get docker bridge ip address
DOCKER_BRIDGE_IP=$(ip addr show {{ docker_if }} | grep 'inet ' | head -n 1| awk '{print $2}' | cut -d/ -f1)



# 8. setup docker dns server to use
cat > /etc/sysconfig/docker-network << EOF
# /etc/sysconfig/docker-network
DOCKER_NETWORK_OPTIONS=--dns=$DOCKER_BRIDGE_IP
EOF



# 9. install and configure dnsmasq
echo "Setup dnsmasq..."

yum -y install dnsmasq

cat > /etc/dnsmasq.d/kuberdock.conf << EOF
interface={{ docker_if }}
no-dhcp-interface={{ docker_if }}
EOF

if grep -E 'etcd.kuberdock|{{ es_host }}' /etc/hosts > /dev/null; then
  # remove etcd and elasticsearch entries from hosts
  grep -Ev 'etcd.kuberdock|{{ es_host }}' /etc/hosts > /etc/hosts.tmp
  mv -f /etc/hosts.tmp /etc/hosts
fi

# add new entries for etcd and elasticsearch to hosts
echo {{ master_ip }} etcd.kuberdock >> /etc/hosts
echo $DOCKER_BRIDGE_IP {{ es_host }} >> /etc/hosts

systemctl enable dnsmasq; systemctl restart dnsmasq



# 10. setup rsyslog forwarding
echo "Reconfiguring rsyslog..."
cat > /etc/rsyslog.d/kuberdock.conf << EOF
*.* @127.0.0.1:5140
EOF

systemctl restart rsyslog



# 11. setup service pods
echo 'Setup service pods...'

# pull images (update if already exists)
systemctl enable docker; systemctl restart docker

docker pull kuberdock/fluentd:1.0 > /dev/null 2>&1 &
docker pull kuberdock/elasticsearch:1.0 > /dev/null 2>&1 &

if [ -d /etc/kubernetes/manifests ]; then
  # remove old pods if exists
  rm -f /etc/kubernetes/manifests/kuberdock-fluentd.manifest
  rm -f /etc/kubernetes/manifests/kuberdock-elasticsearch.manifest

  for c in $(docker ps -a | grep -E 'kuberdock-fluentd.file|kuberdock-elasticsearch.file' | awk '{print $1}'); do
    docker rm -f $c > /dev/null 2>&1
  done
else
  mkdir /etc/kubernetes/manifests
fi

# fix elasticsearch home directory ownership (if ES was running as service)
if [ -d /var/lib/elasticsearch ]; then
  chown -R root:root /var/lib/elasticsearch
fi

cat > /etc/kubernetes/manifests/kuberdock-fluentd.manifest << EOF
version: v1beta2
id: kuberdock-fluentd
containers:
  - name: fluentd
    image: kuberdock/fluentd:1.0
    volumeMounts:
      - name: docker-containers
        mountPath: /var/lib/docker/containers
    ports:
      - name: syslog
        containerPort: 5140
        hostPort: 5140
        protocol: UDP
    env:
      - name: NODENAME
        value: $(hostname)
      - name: ES_HOST
        value: {{ es_host }}
volumes:
  - name: docker-containers
    source:
      hostDir:
        path: /var/lib/docker/containers
EOF

cat > /etc/kubernetes/manifests/kuberdock-elasticsearch.manifest << EOF
apiVersion: v1beta2
id: kuberdock-elasticsearch
containers:
  - name: elasticsearch
    image: kuberdock/elasticsearch:1.0
    ports:
      - name: es-port
        containerPort: 9200
        hostPort: 9200
      - name: es-transport-port
        containerPort: 9300
        hostPort: 9300
    volumeMounts:
      - name: es-persistent-storage
        mountPath: /elasticsearch/data
volumes:
  - name: es-persistent-storage
    source:
      hostDir:
        path: /var/lib/elasticsearch
EOF



# 12. enable services
echo "Starting services..."
systemctl enable flanneld; systemctl restart flanneld
systemctl enable kubelet; systemctl restart kubelet
systemctl enable kube-proxy; systemctl restart kube-proxy
#systemctl enable cadvisor; systemctl restart cadvisor
exit 0
